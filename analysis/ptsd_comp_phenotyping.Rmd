---
title: "Analysis of PTSD Model Simulations"
author: "Andrea Stocco"
date: "2/7/2021"
output:
  html_document:
    code_folding: hide
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Hmisc)
library(tidyverse)      # The foundation
library(ggplot2)        # The foundatio for plots
library(broom)          
library(scales)
library(ggthemes)       # Needs for theme_pander()
library(ggrepel)        # Needed for labels in pie charts

library(viridis)        # For color-blind graded scales (using plasma)
library(ggsci)          # Modern discrete color palettes (D3, JAMA) 
library(RColorBrewer)   # Custom scales (purple scale is used)

library(kableExtra)     # Tables
library(xtable)         # Tables

library(gridExtra)      # Multi plot alignment
library(patchwork)      # Multi-plot alignment

library(ggseg)          # Brain visualization (standard DK atlas)
library(parallel)
library(pROC)

```

# Model and Simulations

This is an analysis of the PTSD model "complete" simulations ("Simulation 3").

## Trajectories to simulate

One of the goals of the model is to simulate the recovery trajectories in PTSD. The classic distinction is between four different trajectories, Resilient, Recovery, Chronic, and Delayed. These trajectories can be stylized as such

```{r fig.width=3, fig.height=3}
time <- -20:60

resilient <- rep(0, length(time))
chronic <- tanh(time/2)
delayed <- time**2 / max(time**2)
#recovery <- chronic - delayed
recovery <- chronic + ((60 - time)**2 / max(time**2)-1)
#recovery <- recovery/max(recovery)

resilient[time <= 0] <- 0
chronic[time <= 0] <- 0
delayed[time <= 0] <- 0
recovery[time <= 0] <- 0

wideal <- tibble(Day = time, 
                Chronic = chronic,
                Delayed = delayed,
                Recovery = recovery,
                Resilient = resilient)
        
ideal <- wideal %>%
  pivot_longer(cols=c("Chronic", "Delayed", "Resilient", "Recovery"),
               values_to = "CTraumatic",
               names_to = "Trajectory")

ideal$Trajectory <- factor(ideal$Trajectory,
                           levels = c("Recovery", "Delayed",
                                  "Resilient",  "Chronic"))

ggplot(ideal, 
       aes(x = Day, 
           y = CTraumatic, 
           col = Trajectory, 
           fill = Trajectory)) +
  #geom_smooth(method="loess", span=0.2) +
  geom_line(size=2, alpha=0.75) +
  ggtitle("Recovery Trajectories") +
  #coord_equal() +
  ylab("Number of Memory Intrusions") +
  geom_vline(data=tibble(), 
             aes(xintercept=-0.35)) +
  scale_color_d3() +
  theme_pander()  +
  xlab("Time") +
  xlim(-5, 60) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.position = c(0.5, 0.7),
        legend.background = element_rect(fill=NA))
```

## Underlying memory model

The underlying memory model is John Anderson's ACT-R. 

```{r fig.width=6, fig.height=6}
# Simulate ACT-R

decay <- function(time, t0=0, rate=.5) {
    if (time <= t0) {
      NA
    } else {
      (time - t0)**(-rate)
    }
}

vdecay <- Vectorize(decay)

time <- seq(1, 100, 1)

t1 = vdecay(time, t0=1)
t2 = vdecay(time, t0=20)
t3 = vdecay(time, t0=55)
t4 = vdecay(time, t0=65)


df <-data.frame(Time=time, 
           trace1 = vdecay(time, t0=1),
           trace2 = vdecay(time, t0=20),
           trace3 = vdecay(time, t0=55),
           trace4 = vdecay(time, t0=65))


ldf <- df %>%
  # mutate(Total = if_else(is.na(trace1), 0, trace1) +
  #          if_else(is.na(trace2), 0, trace2) +
  #          if_else(is.na(trace3), 0, trace3) +
  #          if_else(is.na(trace4), 0, trace4))  %>%
  pivot_longer(cols = c("trace1", "trace2", "trace3", "trace4"),
               names_to = "Trace", values_to="Activation")

ldf$Trace <- fct_recode(ldf$Trace, 
                        "Trace 1" = "trace1",
                        "Trace 2" = "trace2",
                        "Trace 3" = "trace3",
                        "Trace 4" = "trace4")

ldf <- ldf %>%
  add_column(Source="Individual Traces")

t1[is.na(t1)] <-0
t2[is.na(t2)] <-0
t3[is.na(t3)] <-0
t4[is.na(t4)] <-0
total <- log(t1+t2+t3+t4)

totaldf <- tibble(Time=time, 
                  Activation = total,
                  Trace = "Memory",
                  Source="Memory")

ldf <- rbind(ldf, totaldf)

ldf$Source <- factor(ldf$Source, levels = c("Memory", "Individual Traces"))

ggplot(ldf, 
       aes(x=Time, y=Activation, col=Trace)) +
  geom_line(size=1,
            alpha=1) +
            #linetype="dashed") +
  # stat_summary(geom="line", fun = sum, 
  #              col="black", 
  #              alpha=0.5,
  #              position = position_dodge()) +
  # #scale_y_log10() +
  facet_wrap(~ Source, ncol=1,
             scales = "free_y") +
  #ylim(0, 1.6) +
  #ylab(expression(paste("Availability of Memory ", italic(m))))+
  ylab("Activation") +
  scale_color_viridis("", option="plasma", discrete=T, end = .8, direction=-1) +
  theme_pander()
```

### Neurological interpretation

```{r}
brain_model <- tibble(
  region = c("parahippocampal", "entorhinal", "medial orbitofrontal",
             "superior frontal", "paracentral", "precuneus"),
  component = c("LTM", "Emotional Intensity", "Retrieval Control",
                "Context", "Context", "Context")
)

#brain_model$component <- as_factor(brain_model$component)

# brain_model %>%
#  group_by(component) %>%
  # ggplot(aes(fill = component)) +
  # geom_brain(atlas = dk, 
  #            col="white",
  #            show.legend = T
  #            ) +
  # labs(fill="Model\nComponent") +
  # #scale_fill_lancet(na.value="lightgrey") +
  # scale_fill_manual(na.value="lightgrey", 
  #                    values=c("dodgerblue2", "goldenrod1", "firebrick3", 
  #                             "lightskyblue3", "lightskyblue3", "lightskyblue3"),
  #                    breaks = brain_model$component) +
  # coord_sf(xlim = c(750, 1050)) +
  # theme_brain2(text.family = "sans",
  #              text.colour="black") 

```

## Event Distribution

The model lives in a sumulated world in which memories are automatically retrived in response to changes in the environment. These changes, called _events_ occur probabilitistically, with Gamma distrib  a predermined frequen

The distribution of Gamma events is this:

* Frequency: 2.1, 175 (shape, scale)
* Rumination, 6, 100

```{r}
t <- 0:(24*60)
E <- dgamma(t, shape=2.1, rate=1/175)
R <- dgamma(t, shape=6, rate=1/100)

dists <- data.frame(Time=t, Events = E, Rumination = R) %>%
  pivot_longer(names_to = "Type", 
               values_to = "Probability",
               cols=c("Events", "Rumination"))

ggplot(dists, aes(x=Time, y=Probability, col=Type, fill=Type)) +
  geom_line() +
  geom_ribbon(data=dists, aes(x=Time, ymax=Probability, ymin=0), alpha=0.25) +
  scale_x_continuous(breaks=60*c(0, 4, 8, 12, 16, 20),
                     limits=c(0, 24*60),
                     labels=c("8:00am", 
                              "12:00pm",
                              "4:00pm",
                              "8:00pm",
                              "12:00am",
                              "4:00am")) +
  scale_color_aaas() +
  scale_fill_aaas() +
  ggtitle("Probability of Retrievals During the Day") +
  theme_pander()
```

# Load and Transform the Data

First, we are going to load the "aggregated" version of the simulations, with 
the events already aggregated by day. 

```{r load}
CREATE_AGGREGAGATED = F

if (CREATE_AGGREGAGATED) {
  d <- read_csv("../simulations/simulations3.csv")
  #d <- read_csv("simulations3.csv", col_types = cols())
  d <- d %>%
    mutate(Day = floor(Time / (60*60*24)) - 100,
           Hour = floor((d$Time / 3600) %% 24),
           RuminationFrequency=as_factor(RuminationFrequency))
  names(d)[18] <- "W"
  
  d <- d %>% 
    dplyr::select(Run, Day, PTEV, Gamma, PTES, W, 
                  NumAttributes, RuminationFrequency, 
                  MemoryEntropy, ChunkSimilarity, Traumatic, ChunkV) %>%
    filter(Day > -20)
  
  a <- d  %>%
    group_by(Run, Day, PTEV, PTES, Gamma, 
             NumAttributes, RuminationFrequency, W) %>%
    summarise(MemoryEntropy = mean(MemoryEntropy),
              ChunkSimilarity = mean(ChunkSimilarity),
              PTraumatic = mean(Traumatic),
              CTraumatic = sum(Traumatic),
              ChunkV = mean(ChunkV))
  
  write_csv(x=a, file = "../simulations/simulations3_aggregated2.csv")
}

a <- read_csv(gzfile("../simulations/simulations3_aggregated2.csv.gz"),
              col_types = cols())
```

Then, we are going to rename the simulation variables to make them consistent with
the names used in published papers:

```{r rename}
a <- a %>%
  rename(I = PTEV) %>%
  rename(gamma = Gamma) %>%
  rename(A = NumAttributes) %>%
  rename(C = PTES) %>%
  rename(R = RuminationFrequency)
```


## Recovery Trajectories

To identify a recovery trajectory, we need to first define a classification function. Here, we are going to use the following algorithm:

First the model calculates three average values:

1. The mean probability _P(baseline)_ of retrieving intrusive memories in the 10 days 
  preceding the PTE, or _baseline_ period. By definition, this is always zero.
  
2. The mean probability _P(acute)_ of retrieving intrusive memories in the 10 days 
  following the PTE, or during the _acute_ period.
  
3. The mean probability _P(chronic)_ of retrieving intrusive memories in the last 
  ten days of the second month after the PTE (i.e, days 51-60), or the _chronic_ 
  period.
  
Then, the model calculates two statistical tests:

1. A _t_-test between the baseline and acute period, or _acute test_; and 
2. A _t_-test between the acute-period and the chronic period, or _chronic test_.

And finally we apply the following decision tree:

1. If the acute test is significant at p < .05 (Pacute > Pbaseline), then
   
   1.1. If the chronic test is also significant at _p_ < .05, and Pchronic > Pacute, 
         we classify the trajectory as **Delayed**.

   1.2. If the chronic test is significant at _p_ < .05 but 
        _P(chronic)_ < _P(acute)_, then we classify the trajectory as **Recovery**;
        
   1.3. If the chronic test is non-significant, then _P(chronic)_ ~ _P(acute)_ , 
        and we classify the trajectory as **Chronic**.

2. If, instead, the acute test is not significant and _P(acute)_ ~ _P(baseline)_, 
   then:
   
   2.1. If the chronic test is significant at _p_ < .05, then 
        _P(chronic)_ > _P(acute)_, and we classify the trajectory as **Delayed**.
   
   2.2 If the chronic test is also not significant, then 
       _P(chronic)_ ~ _P(baseline)_ ~ _P(acute)_, and we classify the trajectory
       as **Resilient**.

```{r classify}
# Classifies people based on trajectory:
# Chronic = 1
# Recovery =2
# Delay    =3
# resilient= 4
#
classify <- function(days) {
  g1 <- days[10:19]
  g2 <- days[21:30]
  g3 <- days[70:79]
  t12 <- t.test(g1, g2)
  t23 <- t.test(g2, g3)
  category <- 0
  if (!is.na(t12$p.value) & t12$p.value < 0.05) {
    # t1 < t2
    if (!is.na(t23$p.value) & t23$p.value < 0.05) {
      # t2 <> t3

      if (!is.na(t23$statistic) & t23$statistic > 0) {
        # t1 < t2, t2 > 3
        category <- "Recovery" #2  # Recovery
      } else {
        # t1 < t2, t2 < t3
        category <- "Delayed" # 3 # Delayed
      }
    } else {
      # t1 < t2, t2 = t3
      category <- "Chronic" # 1 # Chronic
    }
  } else {
    # t1 == t2
    if (!is.na(t23$p.value) & t23$p.value < 0.05) {
      # t1 == t2, t2 < t3
      category <- "Delayed" #3 # Delayed
    } else {
      # t1 == t2, t2 = t3
      category <- "Resilient" # 4 # Resilient
    }
  }
  category
}

patterns <- a %>%
  group_by(Run, I, C, A, R, W, gamma) %>%
  dplyr::summarize(Trajectory = classify(PTraumatic))

patterns <- patterns %>% ungroup
```

Finally, let's assign each data point in the main averages with the corresponding trajectory, and re-order the trajectories in order of severity:

```{r}
a <- inner_join(a, patterns)
a$Trajectory <- factor(a$Trajectory,
                      levels = c("Recovery", "Delayed", "Resilient",  "Chronic"))
```
## Chronic Probability of Traumatic Intrusions

The other "behavioral" measure is the incidence of traumatic memory intrusions. As an aggregate measure, we will consider only the averages in the "Chronic" period, which we will be taking as indicative of long-term consequences of the PTSD.

```{r}
traumatic <- a %>%
  filter(Day > 1) %>%
  group_by(Run, I, C, R, W, gamma) %>%
  dplyr::summarize(PTraumatic = mean(PTraumatic),
            CTraumatic = sum(CTraumatic))
```

At this point, we can visualize the mean aggregated timecourse of intrusions for each trajectory.


```{r, fig.width=4, fig.height=4}

TOTAL <- 28800  # Param combos

traj_total <- a %>%
  filter(Day == 30) %>%
  group_by(Trajectory) %>%
  summarise(N = length(CTraumatic),
            Prop = length(CTraumatic)/TOTAL,
            MeanC = mean(CTraumatic))

ggplot(filter(a, I != 1), 
       aes(x=Day, y=CTraumatic, col=Trajectory, fill=Trajectory)) +
  ggtitle("Memory Intrusions by Trajectory") +
  geom_text(data=traj_total, aes(x= 30, y = MeanC, 
                                 label=percent(Prop, accuracy = 0.1)),
            vjust=-.5, show.legend=F) +
  
  stat_summary(fun.data = mean_se, geom="line") +
  stat_summary(fun.data = mean_cl_normal, 
               geom="ribbon", alpha = 0.25, col=NA) +
  coord_cartesian(ylim=c(0, 15), xlim=c(-5, 59)) +
  ylab("Total Number of IMs") +
  scale_color_d3() +
  scale_fill_d3() +
  theme_pander() +
  theme(legend.background = element_rect(fill = NA),
        legend.position = c(0.8, 0.25))
```

Better graph for R21

```{r, fig.width=3, fig.height=4}
TOTAL <- 28800  # Param combos

traj_total <- a %>%
  filter(Day == 30) %>%
  group_by(Trajectory) %>%
  summarise(N = length(CTraumatic),
            Prop = length(CTraumatic)/TOTAL,
            MeanC = mean(CTraumatic))

minia <- a %>%
  filter(I > 1)

minia$I <- as.factor(minia$I)
ggplot(minia, 
       aes(x=Day, y=CTraumatic, col=I, fill=I)) +
  ggtitle("Number of IMs per Day") +
  facet_wrap(~ C, ncol=2,
             labeller = label_bquote(
               cols = italic(C) == .(C))) +
  # geom_text(data=traj_total, aes(x= 30, y = MeanC, 
  #                                label=percent(Prop, accuracy = 0.1)),
  #           vjust=-.5, show.legend=F) +
  # 
  stat_summary(fun.data = mean_se, geom="line") +
  stat_summary(fun.data = mean_cl_normal, 
               geom="ribbon", alpha = 0.25, col=NA) +
  coord_cartesian(ylim=c(0, 20), xlim=c(-5, 59)) +
  ylab("Total Number of IMs") +
  scale_color_viridis(discrete=T, end=0.8, option = "inferno") +
  scale_fill_viridis(discrete=T, end=0.8, option = "inferno") +
  theme_pander() +
  theme(legend.background = element_rect(fill = NA),
        legend.position = "bottom")
```


The results indicate that our classification was correct; the trajectories behave as planned.

## Finding a baseline to compare the model to existing literature

Since we have no idea how typical the model parameters are of the existing patients and their cases, we need to provide some sort of baselines. To do so, we will use the proportion of trajectories that were given by Galatzer-Levy and Bonanno in their meta-analysis. They are:

* Resilient, 0.657
* Recovery, 0.208 
* Chronic, 0.106
* Delayed, 0.089


Note that they do not normalize to one, due to the lack of some trajectories in some studies. To use them, we will need to make sure to normalized them first, which can be done at time of testing.

```{r}
T_observed <- c(0.657, 0.208, 0.106, 0.089)
names(T_observed) <- c("Resilient", "Recovery", "Chronic", "Delayed")
```

We can now plot the percentages and the patterns by trajectory:

```{r}
props <- patterns %>%
  group_by(Trajectory, 
           I, 
           gamma, 
           C, 
           W,
           R,
           #A,
           ) %>%
  dplyr::summarize(N = length(Trajectory)) %>%
  dplyr::mutate(Prop = N/50)

props %>%
  pivot_wider(id_cols=c(I, 
                        gamma, 
                        C,
                        W,
                        R,
                        #A,
                        ), 
              names_from = Trajectory,
              values_from = N,
              values_fill=0) -> test

mychi <- function(vals) {
  chisq.test(vals, 
             p=T_observed, 
             rescale.p=T)$statistic
}

mycorr <- function(vals) {
  cor(vals, T_observed)
}

myrmse <- function(vals) {
  v <- 50 * (T_observed/sum(T_observed))
  sqrt(mean((vals - v)^2))
}


lprops <- test %>%
  pivot_longer(cols=c("Resilient", "Recovery", "Chronic", "Delayed"),
               names_to = "Trajectory",
               values_to = "N")

lprops$Trajectory <- factor(lprops$Trajectory,
                      levels = c("Recovery", "Delayed", "Resilient",  "Chronic"))

# Suppresses X-squared approximation warnings
old.warn <- options()$warn
options(warn = -1)

atest <- lprops %>%
  group_by(I, 
           gamma, 
           C, 
           W,
           R,
           #A,
           ) %>%
  summarise(Chi = mychi(N), r=mycorr(N), RMSE=myrmse(N))

# resets warnings
options(warn = old.warn)

test <- inner_join(test, atest)
```

Now, we can identfy the baseline as the condition with the minimum $\chi^2$-squared test (or the minimum RMSE, they are the same)

This shows how correlation coefficient and $\chi^2$ are related, while the RMSE does not really capture either of them:

```{r}
ggplot(test, aes(x=Chi, y=r, col=RMSE)) + 
  geom_point(size=4, alpha=0.5) + 
  scale_color_viridis(option="plasma")+
  ylab(expression(italic(r))) +
  xlab(expression(chi^2)) +
  ggtitle("Relationship between Error Measures in Trajectories") +
  theme_pander()
```

Now, let's identify a "baseline" condition---the one that most resembles the distribution of trajectories identified by Galatzer-Levy. 

```{r}
baseline <- test %>%
  filter(Chi == min(test$Chi))

baseline %>%
  kable(digits=3) %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```


And here is a visual representation of the four trajectories at that baseline (smoothed, because there is too much day-to-day variance in this small sample).

```{r fig.width=5, fig.height=3.5}
baseline_a <- a %>%
  filter(I == baseline$I,
         W == baseline$W,
         C == baseline$C,
         R == baseline$R,
         gamma == baseline$gamma,
         )

base_total <- baseline_a %>%
  filter(Day == 15) %>%
  group_by(Trajectory) %>%
  summarise(N = length(CTraumatic),
            Prop = length(CTraumatic)/100,
            MeanC = mean(CTraumatic))

         
ggplot(baseline_a, 
       aes(x=Day, y=CTraumatic, col=Trajectory, fill=Trajectory)) +
  geom_smooth(method="loess", span=0.2) +
  ggtitle("Trajectories for Best-Fitting Parameters") +
  scale_color_d3() +
  geom_text(data=base_total, aes(x= c(25, 10, 20, 10), y = MeanC, 
                                 label=percent(Prop, accuracy = 0.1)),
            vjust=-.5, show.legend=F) +
  ylab("Total Number of Memory Intrusions") +
  geom_vline(data=props, aes(xintercept=-0.25)) +
  scale_fill_d3() +
  theme_pander() 
```

The Chonic and Delayed trajectories, being the least common, exhibit significant ups and downs, hence the need for Loess smoothing. Nonetheless, the trajectories remain visible and clearly identifiable.

But how good is the baseline condition, in terms of being closed to Galatzer-Levy's estimated pooled values? We can just examine the significance of the $\chi^2$ test.

```{r}
baseline_vals <- baseline[c("Resilient", "Recovery", "Chronic", "Delayed")]
chi <- chisq.test(baseline_vals, p=T_observed, rescale.p = T)
```

The result is not too bad---A bit dissimilar, but not too far either, and not statistically significant ($\chi^2$ = `r round(chi$statistic, 6)`, _p_ = `r round(chi$p.value, 6)`)).

The main difference is that the delayed onset trajectory has a higher proportion than in the review. However, Galatzer-Levy and Bonanno themselves note that the resilient (M= 0.66) delayed onset trajectories (M = 0.099) has a higher prevalence in studies of single-point event. If these corrected prevalences are taken into account, the chi squared reduces further.

```{r, fig.width=4, fig.height=4}
miniprops <- lprops %>%
  ungroup() %>%
  filter(I > 1) %>%
  group_by(I, C, Trajectory) %>%
  summarize(N = sum(N))
ggplot(data=miniprops, aes(x="", y=N/100, fill=Trajectory)) +
  geom_bar(stat = "identity", col="white", width=1) +
  #facet_grid(C~I, labeller = label_both) +
  facet_grid(I~C, labeller = label_bquote(
    cols = italic(C) == .(C),
    rows = italic(I)[PTE] == .(I))) +
  coord_polar("y", start=0) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 2L)) +
  scale_fill_d3() +
  scale_color_d3() +
  xlab("PTE Intensity") +
  ylab("Contextual Similarity") +
  ggtitle("Proportions of Trajectories") +
  geom_text_repel(aes(label=percent(N/1800, .1)), col="black",
            position=position_stack(vjust=0.5 ), size=3)+
  theme_pander() +
  labs(fill="") +
  theme(axis.ticks = element_blank(), 
        axis.text.y = element_blank(),
        axis.text.x = element_blank(),
        legend.position = "bottom",
        ) 
```

# Predictions

## Forward prediction

Forward predictions assume we know the model's initial conditions. They are used to predict the final outcome.

### Predictions About Trajectories

```{r}
grand <- a %>%
  group_by(W, I, C, gamma, R, Trajectory) %>%
  summarize(Prob = length(Trajectory) / 100)

grand_f <- grand %>%
  group_by(W, I, C, gamma, R) %>%
  filter(Prob == max(Prob)) %>%
  rename(PredictedTrajectory = Trajectory)

forward_prediction <- full_join(patterns, grand_f) %>%
  mutate(Accuracy = if_else(Trajectory == PredictedTrajectory, 1, 0))
```
Now, visualization

```{r}
forward_prediction %>%
  filter(I > 1) %>%
  group_by(Trajectory) %>%
  summarise(Accuracy = mean(Accuracy)) 

forward_prediction %>%
  filter(I > 1) %>%
  summarise(Accuracy = mean(Accuracy)) 
```


Visualization

```{r}
forward_prediction$Trajectory <- factor(forward_prediction$Trajectory,
                                        levels = c("Resilient", "Recovery", "Chronic", "Delayed"))

forward_prediction$PredictedTrajectory <- factor(forward_prediction$PredictedTrajectory,
                                                 levels = c("Resilient", "Recovery", "Chronic", "Delayed"))

forward_percentages <- forward_prediction %>%
  filter(I > 1) %>%
  #group_by(I, C, R, W, gamma) %>%
  #mutate(Count = length(Trajectory)) %>%
  ungroup() %>%
  group_by(Trajectory, PredictedTrajectory) %>%
  summarize(Percent = length(Prob)/nrow(filter(forward_prediction, I> 1))) 
  #ungroup() %>%
  #group_by(Trajectory) %>%
  #summarise(Percent = length(Count) / mean(Count))

ggplot(forward_percentages, aes(x=PredictedTrajectory, y=Trajectory, fill=Percent)) +
  geom_tile(col="white") +
  scale_fill_viridis(option="inferno", end=0.8) +
  ggtitle("Confusion Matrix for Predicted Trajectories") +
  geom_text(aes(label = percent(Percent, .1)), col="white") +
  theme_pander()
```

#### AUC

```{r}
forward_prediction <- forward_prediction %>%
  
  mutate(ResilientTrajectory = if_else(Trajectory == "Resilient", 
                                           1, 
                                           0),
         ResilientPredicted = if_else(PredictedTrajectory == "Resilient",
                                           1, 
                                           0),
         RecoveryTrajectory = if_else(Trajectory == "Recovery", 
                                           1, 
                                           0),
         RecoveryPredicted = if_else(PredictedTrajectory == "Recovery", 
                                           1, 
                                           0),
         ChronicTrajectory = if_else(Trajectory == "Chronic", 
                                           1, 
                                           0),
         ChronicPredicted = if_else(PredictedTrajectory == "Chronic", 
                                           1, 
                                           0),
         DelayedTrajectory = if_else(Trajectory == "Delayed", 
                                           1, 
                                           0),
         DelayedPredicted = if_else(PredictedTrajectory == "Delayed", 
                                           1, 
                                           0)
         ) 

roc_resilient <- roc(forward_prediction$ResilientTrajectory, 
                     forward_prediction$ResilientPredicted)

roc_recovery <- roc(forward_prediction$RecoveryTrajectory, 
                    forward_prediction$RecoveryPredicted)

roc_chronic <- roc(forward_prediction$ChronicTrajectory, 
                   forward_prediction$ChronicPredicted)

roc_delayed <- roc(forward_prediction$DelayedTrajectory, 
                   forward_prediction$DelayedPredicted)
```

```{r fig.width=4, fig.height=4}
p1 <- ggroc(roc_resilient, col="red") +
  geom_point(aes(y=roc_resilient$sensitivities, 
                 x=roc_resilient$specificities), col="red", size=4, 
             
             alpha=.5) +
  ggtitle("Resilient") +
  xlab("") + ylab("") + 
  geom_segment(aes(x = 1, xend = 0, 
                   y = 0, yend = 1), 
               color="grey", linetype="dashed") +
  geom_text(aes(x = 0.5, y = 0.25, 
                label = paste("AUC = ", 
                              round(roc_resilient$auc, 2)))) +
  theme_pander()


p2 <- ggroc(roc_recovery, col="red") +
  geom_point(aes(y=roc_recovery$sensitivities, 
                 x=roc_recovery$specificities), col="red", size=4, 
             
             alpha=.5) +
  ggtitle("Recovery") +
  xlab("") + ylab("") + 
  geom_segment(aes(x = 1, xend = 0, 
                   y = 0, yend = 1), 
               color="grey", linetype="dashed") +
  geom_text(aes(x = 0.5, y = 0.25, 
                label = paste("AUC = ", 
                              round(roc_recovery$auc, 2)))) +
  theme_pander()


p3 <- ggroc(roc_chronic, col="red") +
  geom_point(aes(y=roc_chronic$sensitivities, 
                 x=roc_chronic$specificities), col="red", size=4, 
             
             alpha=.5) +
  ggtitle("Chronic") +
  xlab("Specificity (TNR)") + ylab("Sensitivity (TPR)") + 
  geom_segment(aes(x = 1, xend = 0, 
                   y = 0, yend = 1), 
               color="grey", linetype="dashed") +
  geom_text(aes(x = 0.5, y = 0.25, 
                label = paste("AUC = ", 
                              round(roc_chronic$auc, 2)))) +
  theme_pander()


p4 <- ggroc(roc_delayed, col="red") +
  geom_point(aes(y=roc_delayed$sensitivities, 
                 x=roc_delayed$specificities), 
             col="red", size=4, 
             
             alpha=.5) +
  ggtitle("Delayed") +
  xlab("") + ylab("") + 
  geom_segment(aes(x = 1, xend = 0, 
                   y = 0, yend = 1), 
               color="grey", linetype="dashed") +
  geom_text(aes(x = 0.5, y = 0.25, 
                label = paste("AUC = ", 
                              round(roc_delayed$auc, 2)))) +
  theme_pander()


(p1 | p2) / (p3 + p4)
```

#### Single-plot of AUC

```{r, fig.width=3, fig.height=3}
aucs <- c(roc_resilient$auc,
          roc_recovery$auc,
          roc_chronic$auc,
          roc_delayed$auc)

xs <- c(roc_resilient$specificities,
       roc_recovery$specificities,
       roc_chronic$specificities,
       roc_delayed$specificities)

ys <- c(roc_resilient$sensitivities,
       roc_recovery$sensitivities,
       roc_chronic$sensitivities,
       roc_delayed$sensitivities)

wroc <- tibble(Trajectories = rep(c("Resilient", "Recovery", "Chronic", "Delayed"), each=3),
               x = xs,
               y = ys)

wauc <- tibble(Trajectories = rep(c("Resilient", "Recovery", "Chronic", "Delayed"), each=1),
               x = xs[c(2, 5, 8, 11)],
               y = ys[c(2, 5, 8, 11)],
               AUC = aucs)

wroc$Trajectories <- factor(wroc$Trajectories,
                            levels = c("Recovery", 
                                       "Delayed", 
                                       "Resilient", 
                                       "Chronic"))

ggplot(wroc, aes(x=x, y=y, col=Trajectories)) +
  geom_line() +
  geom_point(size=3) +
  scale_color_d3() +
  ylab("Sensitivity (True Positive Rate)") +
  xlab("Specificity (True Negative Rate)") +
  scale_x_reverse() +
  ggtitle("Prediction Accuracy\nfor Trajectories") +
  geom_text_repel(data=wauc, aes(x=x, y=y), 
                   label=paste("AUC =", round(wauc$AUC, 2))) +
  geom_abline(slope=1, intercept = 1, col="grey", linetype = "dashed") +
  theme_pander() +
  theme(legend.position = c(0.8, 0.3))
```

### Predictions About Intrusions

```{r}
MyMode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

lastweek <- a %>%
  filter(Day > 52, I > 1) %>%
  group_by(W, I, gamma, R, C, Run, A, Trajectory) %>%
  summarise(NumberIntrusions = sum(CTraumatic))

lastweekp <- lastweek %>% 
  group_by(W, I, gamma, R, C) %>%
  summarise(PredictedNumIntrusionsMean = mean(NumberIntrusions),
            PredictedNumIntrusionsMedian = median(NumberIntrusions),
            PredictedNumIntrusionsMode = MyMode(NumberIntrusions))
```

Now, we can re-join the data:

```{r}
lastweek_comp <- full_join(lastweek, lastweekp)
```

And now, let's calculate the predictions:

```{r fig.width=3, fig.height=3}
lastweek_comp_nozero <- filter(lastweek_comp, 
                               NumberIntrusions > 10,
                               PredictedNumIntrusionsMode > 10)

smmry <- tibble(r = round(
  cor(lastweek_comp_nozero$PredictedNumIntrusionsMode,
      lastweek_comp_nozero$NumberIntrusions),
  2))

ggplot(lastweek_comp_nozero, aes(x=PredictedNumIntrusionsMode,
                          y =NumberIntrusions)) +
  #geom_count(alpha = 0.15, size=0.5, col="navy",
  #            position="jitter") +
  geom_bin2d(bins=12, col="white") +
  scale_fill_viridis() +
  scale_size_area() +
  xlab("Num Predicted (Week 8)") +
  ylab("Num Observed (Week 8)") +
  ggtitle("Prediction Accuracy\nfor Intrusions") +
  geom_smooth(method = "lm", col="red") +
  geom_text(data=smmry, x = 220, y = 50, 
            color="red", label=paste("r =",  smmry$r)) +
  coord_equal() +
  theme_pander()
```

```{r fig.width=5.5, fig.height=3}
p1 <- ggplot(lastweek_comp_nozero, aes(x=PredictedNumIntrusionsMode,
                          y =NumberIntrusions)) +
  geom_count(alpha = 0.15, size=0.5, col="grey",
              position="jitter") +
  #geom_bin2d(bins=12, col="white") +
  #scale_fill_viridis() +
  scale_size_area() +
  xlab("Num Predicted (Week 8)") +
  ylab("Num Observed (Week 8)") +
  ggtitle("(A) Accuracy\nfor Intrusions") +
  geom_smooth(method = "lm", col="red") +
  geom_text(data=smmry, x = 220, y = 50, 
            color="red", label=paste("r =",  smmry$r)) +
  coord_equal() +
  xlim(0,300) +
  theme_pander()

p2 <- ggplot(wroc, aes(x=x, y=y, col=Trajectories)) +
  geom_line() +
  geom_point(size=3) +
  scale_color_d3() +
  ylab("Sensitivity (True Pos. Rate)") +
  xlab("Specificity (True Neg. Rate)") +
  scale_x_reverse() +
  coord_equal() +
  ggtitle("(B)  Accuracy\nfor Trajectories") +
  geom_text_repel(data=wauc, aes(x=x, y=y), 
                   label=paste("AUC =", round(wauc$AUC, 2))) +
  geom_abline(slope=1, intercept = 1, col="grey", linetype = "dashed") +
  theme_pander() +
  theme(legend.position = c(0.8, 0.3),
        legend.background = element_rect(fill=NA))

p1 | p2
```

## Backward Predictions

```{r}
intermediate <- a %>%
  filter(Day > 0) %>%
  filter(I != 1) %>%
  mutate(Condition = paste("(", I, 
                           ", ",  C, 
                           ", ", W, ")", sep="")) %>%
  group_by(Day, I, C, gamma, W, R, Condition)  %>%
  summarise(Intrusions=mean(CTraumatic),
            SDIntrusions=sd(CTraumatic),) 
```

```{r fig.width=10, fig.height=10}
ggplot(intermediate, 
       aes(x=Day, y=Intrusions, col=as.factor(gamma))) +
  geom_point() +
  ylim(0, 50) +
  facet_wrap(~ Condition) + #label=label_both) +
  geom_vline(data=props, aes(xintercept=-0.15)) +
  theme_pander()
```


Now, let's build the probability table

```{r}
get_prob_by_day_5 <- function(n, day, Intensity, Context, WM, gammaval, Rval) {
    intermediate %>%
    filter(Day == day,
           W == WM,
           I == Intensity,
           C == Context,
           R == Rval, 
           gamma == gammaval) -> sub
    m <- sub$Intrusions
    sd <- sub$SDIntrusions
    dnorm(n, m, sd)
}

CREATE_LOGLIKELIHOODS <- F

if (CREATE_LOGLIKELIHOODS) {
  vals <- NULL
  for (n in 0:max(a$CTraumatic)) {
    print(n)
    for (d in unique(intermediate$Day)) {
      for (i in unique(intermediate$I)) {
        for (c in unique(intermediate$C)) {
          for (w in unique(intermediate$W)) {
            for (g in unique(intermediate$gamma)) {
              for (r in unique(intermediate$R)) {
                p <- get_prob_by_day_5(n, d, i, c, w, g, r)
                result <- c(n, d, i, c, w, g, r, p)
                if (is.null(vals)) {
                  vals <- result
                } else {
                  vals <- rbind(vals, result)
                }
              }
            }
          }
        }
      }
    }
  }
  

  vals <- as_tibble(vals)
  names(vals) <- c("N", "Day", "I", "C", "W", "gamma", "R", "Probability")
  
  #Replace probabilities=0 with very small values 
  vals$Probability[vals$Probability < 1e-200] <- 1e-200
  vals$Probability[vals$Probability > 1] <- 1
  vals$LogLikelihood <- log(vals$Probability)
  write_tsv(vals, "loglikelihoods5_60.tsv")
} else {
  vals <- read_tsv("loglikelihoods5_60.tsv",
                   col_types = cols(
                     N = col_double(),
                     Day = col_double(),
                     I = col_double(),
                     C = col_double(),
                     W = col_double(),
                     gamma = col_double(),
                     R = col_double(),
                     Probability = col_double(),
                     LogLikelihood = col_double()
                   ))
}
```


```{r}
k <- NULL
parameters_mle <- function(predicted) {
  filtered <- NULL
  day <- 1
  #print(predicted)
  for (n in as_vector(predicted)) {
    sub <- vals %>%
      filter(Day == day,  N == n)
    if (is.null(filtered)) {
      filtered <- sub
    } else {
      filtered <- rbind(filtered, sub)
    }
#    print(c(day, n, dim(filtered)))
    
    day <- day + 1
  }
  filtered
  likelihoods <- filtered %>%
    group_by(I, C, W, gamma, R) %>%
    summarise(LogLikelihood = sum(LogLikelihood))
  filter(likelihoods, LogLikelihood == max(likelihoods$LogLikelihood))
}
```


Example run:

```{r}
test <- a %>% 
  filter(Day > 0, Day <= 20) %>%
  filter(W == 4,  
         C == 0.25, 
         gamma == 0.9, 
         I == 60, 
         R == 20, 
         A == 4, 
         Run == 2) %>% 
  dplyr::select(CTraumatic)
parameters_mle(test)
```

Now let's analyze all the runs of the model, and create the predicted parameter values for each

```{r}
mle_prediction <- NULL

CREATE_MLE <- F

if (CREATE_MLE) {
  options(dplyr.summarise.inform = FALSE)
  for (maxday in c(1,10,20)) { #max(a$Day)) {
    a_sub <- a %>%
      filter(Day > 0, Day <= maxday, I > 1)
    
    
    jj <- 1
    
    for (i in unique(a_sub$I)) {
      for (c in unique(a_sub$C)) {
        for (w in unique(a_sub$W)) {
          for (g in unique(a_sub$gamma)) {
            for (r in unique(a_sub$R)) {
              for (att in unique(a_sub$A)) {
                print(c(maxday, i, c, w, g, r, att))
                for (j in unique(a_sub$Run)) {
                  
                  values <- a_sub %>%
                    filter(W == w,  
                           C == c, 
                           gamma == g, 
                           I == i, 
                           R == r, 
                           A == att, 
                           Run == j) %>% 
                    dplyr::select(CTraumatic)
                  prediction <- as_vector(parameters_mle(values)[1,])
                  
                  observed <- tibble(Parameters = c("I", "C", "W", "gamma", "R"),
                                     Type = "Observed",
                                     MaxDay = maxday,
                                     Value = c(i, c, w, g, r),
                                     LogLikelihood = prediction[6],
                                     Run = jj)
                  
                  predicted  <- tibble(Parameters = c("I", "C", "W", "gamma", "R"),
                                       Type = "Prediction",
                                       MaxDay = maxday,
                                       Value = prediction[1:5],
                                       LogLikelihood = prediction[6],
                                       Run = jj)
                  
                  set <- rbind(observed, predicted)
                  #print("Done")
                  
                  if (is.null(mle_prediction)) {
                    mle_prediction <- set
                  } else {
                    mle_prediction <- rbind(mle_prediction, set)
                  }
                  jj <- jj+1
                }
              }
            }
          }
        }
      }
    }
  }
  options(dplyr.summarise.inform = T)
  write_csv(mle_prediction, "mle_predictions5_alldays.csv")
} else {
  mle_predictions <- read_csv(gzfile("mle_predictions5_alldays.csv.gz"),
                              col_types = cols(
                                Parameters = col_character(),
                                Type = col_character(),
                                MaxDay = col_double(),
                                Value = col_double(),
                                LogLikelihood = col_double(),
                                Run = col_double()
                              ))
}
```


```{r fig.width=6, fig.height=10}

mle_predictions <- mle_predictions %>%
  group_by(Parameters, MaxDay) %>%
  mutate(ScaledValue = Value / max(Value))

wmle_predictions <- mle_predictions %>%
  dplyr::select(-ScaledValue) %>%
  pivot_wider(values_from = "Value", names_from = "Type")

ggplot(wmle_predictions, aes(x=Prediction, y=Observed)) +
  geom_count(alpha=.75, aes(size = ..prop.., col=..prop..)) +
  geom_smooth(method="lm", col="red") +
  facet_wrap(MaxDay~Parameters, scales="free", ncol=5) +
  theme_pander()

```

```{r fig.width=10, fig.height=12}
newbackpred <- wmle_predictions %>%
  mutate(SObserved = as.character(Observed),
         SPredicted = as.character(Prediction)) %>%
  ungroup() %>%
  group_by(Parameters, MaxDay) %>%
  mutate(Count = length(Run)) %>%
  ungroup() %>%
  group_by(Parameters, SPredicted, SObserved, MaxDay) %>%
  summarise(Percent = length(Count) / mean(Count))
  
newbackpred$SPredicted[newbackpred$SPredicted == "4"] <- "04"
newbackpred$SPredicted[newbackpred$SPredicted == "8"] <- "08"

newbackpred$SObserved[newbackpred$SObserved == "4"] <- "04"
newbackpred$SObserved[newbackpred$SObserved == "8"] <- "08"


ggplot(newbackpred, aes(x=SPredicted, y=SObserved, fill=Percent)) +
  geom_tile(col = "white") +
  scale_fill_viridis(option="magma", end=0.8) +
  #geom_smooth(method="lm", col="red") +
  facet_wrap(MaxDay~Parameters, scales="free", ncol=5) +
  geom_text(aes(label = percent(Percent, .1)), col="white") +

  theme_pander()
```

### AUCs

We need to create AUCs for every parameter.

```{r}
aucs <- NULL
for (param in unique(newbackpred$Parameters))  {
  for(mday in unique(newbackpred$MaxDay))  {
    sub <- filter(newbackpred, Parameters == param, MaxDay == mday)
    sub$SPredicted <- as.numeric(sub$SPredicted)
    sub$SObserved <- as.numeric(sub$SObserved)
    mroc <- multiclass.roc(sub$SObserved, sub$SPredicted)
    auc <- as.numeric(mroc$auc)
    partial <- tibble(Param = param, MaxDay = mday, AUC=auc)
    if (is.null(aucs)) {
      aucs <- partial
    } else {
      aucs <- cbind(aucs, partial)
    }
  }
}
  
```

```{r}
wmle_predictions %>%
  ungroup() %>%
  mutate(Accuracy = if_else(Prediction == Observed, 1, 0)) %>%
  group_by(Parameters, MaxDay) %>%
  summarise(Accuracy = mean(Accuracy),
            RMSE = sqrt(mean((Prediction - Observed)**2)),
            Correlation = cor.test(Prediction, Observed)$estimate) -> accuracies

accuracies
```

```{r fig.height=10, fig.width=5}
accuracies$Chance = rep(c(1/4, 1/3, 1/3, 1/2, 1/3), each = 7)

ggplot(accuracies, aes(x=Parameters, y=Accuracy, fill=as_factor(MaxDay))) +
  scale_fill_aaas() +
  facet_wrap(MaxDay ~ Parameters, ncol=5, drop=TRUE, scales = "free_x") +
  geom_col(col="lightgrey") +
  geom_hline(aes(yintercept = Chance), linetype="dashed") +
  ylim(0, 0.75) +
  theme_pander()

#ggsave("alia.png")

```

```{r}
wmle_predictions %>% 
  group_by(Parameters) %>% 
  summarise(Predicted_SD = sd(Prediction), 
            Observed_SD = sd(Observed), 
            Predicted_Mean = mean(Prediction),
            Observed_Mean = mean(Observed), 
            )
```

